\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}       % Кодировка
\usepackage[T2A]{fontenc}         % Шрифты
\usepackage[russian]{babel}       % Русский язык
\usepackage{geometry}             % Поля страницы
\usepackage{graphicx}             % Для вставки картинок
\usepackage{amsmath}              % Математические формулы
\usepackage{amssymb}              % Математические символы
\usepackage{fancyhdr}             % Колонтитулы
\usepackage{hyperref}             % Гиперссылки в оглавлении
\usepackage{enumitem}             % Настройка списков
\usepackage{indentfirst}          % Отступ в первом абзаце

\usepackage{float}                % Позволяет использовать [H]
\usepackage{csquotes}             % Для правильных кавычек \enquote

\geometry{left=2cm, right=1.5cm, top=2cm, bottom=2cm}

\pagestyle{fancy}
\fancyhf{} % Очистка текущих колонтитулов
\fancyhead[L]{Лабораторные работы по ИП} % Верхний колонтитул слева
\fancyfoot[C]{\thepage} % Номер страницы снизу по центру
\renewcommand{\headrulewidth}{0.4pt} % Линия под верхним колонтитулом

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Лабораторные работы по ИП},
}

\begin{document}

\begin{titlepage}
\begin{center}
\bfseries

{\Large Московский авиационный институт\\ (национальный исследовательский университет)

}

\vspace{48pt}

{\large Факультет информационных технологий и прикладной математики
}

\vspace{36pt}

{\large Кафедра вычислительной математики и~программирования

}


\vspace{48pt}

Лабораторные работы по курсу \enquote{Информационный поиск}

\end{center}

\vspace{72pt}

\begin{flushright}
\begin{tabular}{rl}
Студент: & Д.\,А. Баталин \\
Преподаватель: & А.\,А. Кухтичев \\
Группа: & М8О-412Б \\
Дата: & \\
Оценка: & \\
Подпись: & \\
\end{tabular}
\end{flushright}

\vfill

\begin{center}
\bfseries
Москва, \the\year
\end{center}
\end{titlepage}

% --- Оглавление ---
\tableofcontents
\newpage

% --- Основной текст ---

\section{ЛР №1. Добыча корпуса документов}

\subsection{Источники данных (Source Data)}
Для построения учебного поискового индекса был выбран корпус документов тематики \enquote{Информационные технологии}. Источниками послужили два ресурса с принципиально разной структурой верстки:
\begin{itemize}
    \item \textbf{Habr.com} - современный ресурс с сложной DOM-структурой, обилием JavaScript-скриптов и CSS-стилей.
    \item \textbf{OpenNet.ru} - новостной ресурс \enquote{старой школы} с минималистичной HTML-версткой.
\end{itemize}
Всего для анализа было скачано 6 документов (по 3 с каждого источника).

\subsection{Характеристики документов}
В ходе анализа \enquote{сырых} данных были выявлены следующие особенности:
\begin{itemize}
    \item \textbf{Habr.com}: Документы используют стандарт HTML5. В коде содержится значительное количество мета-информации (OpenGraph теги og:title, og:description, JSON-LD разметка для поисковиков). Большую часть объема файла занимают технические данные (inline SVG иконки, скрипты React/Vue, CSS классы).
    \item \textbf{OpenNet.ru}: Документы имеют более простую структуру. Мета-информация представлена минимально (keywords, description).
    \item \textbf{Кодировка}: Habr использует UTF-8, OpenNet часто отдает контент в KOI8-R или CP1251, что потребовало автоматического определения кодировки при скачивании.
\end{itemize}

\subsection{Статистические данные и анализ корпуса}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Источник} & \textbf{Кол-во док.} & \textbf{Ср. размер Raw} & \textbf{Ср. размер Text} & \textbf{Доля текста} \\ \hline
    Habr     & 3 & 304.62 & 14.88 & 4.76\%  \\ \hline
    OpenNet  & 3 & 161.43 & 49.99 & 30.96\% \\ \hline
    \textbf{Итого} & \textbf{6} & \textbf{233.02} & \textbf{32.24} & \textbf{13.83\%} \\ \hline
    \end{tabular}
    \caption{Статистика корпуса документов}
\end{table}

\newpage

\section{ЛР №2. Поисковый робот}

\subsection{Цель работы}
Разработка автоматизированного сборщика (краулера) текстовых документов для формирования корпуса данных объемом более 30 000 статей.

\subsection{Архитектура решения}
Робот реализован на языке Python с использованием библиотек \texttt{requests} и \texttt{pymongo}.

Компоненты системы:
\begin{itemize}
    \item \textbf{Конфигурационный файл} (config.yaml). Содержит настройки подключения к БД, параметры задержек, целевые диапазоны ID статей и шаблоны URL.
    \item \textbf{State-менеджер}. Робот сохраняет текущий прогресс (ID последней обработанной статьи) в JSON-файл (crawler\_state\_source.json). Это позволяет безопасно прерывать работу через ctrl+c и возобновлять её с места остановки.
    \item \textbf{Параллельная работа}: Архитектура позволяет запускать несколько независимых экземпляров робота для разных источников, которые пишут в одну базу данных.
\end{itemize}

\subsection{Структура хранимых данных}
В коллекции \texttt{raw\_docs} базы данных MongoDB сохраняются документы следующего формата:
\begin{itemize}
    \item \textbf{url}: Нормализованный URL документа (используется как уникальный индекс unique=True во избежание дубликатов).
    \item \textbf{raw\_html}: Полный HTML-код страницы без предварительной обработки.
    \item \textbf{source}: Метка источника (habr или opennet).
    \item \textbf{crawled\_at}: Unix timestamp времени скачивания.
\end{itemize}
Для реализации обновления (переобкачки) используется метод \texttt{update\_one} с параметром \texttt{upsert=True}: если документ с таким URL уже существует, он обновляется, если нет - создается новый.

\subsection{Проблемы и решения}
\textbf{Проблема блокировок:}\\
В ходе сбора данных с источника OpenNet.ru IP-адрес робота был заблокирован (получен ответ с текстом \enquote{Flood detected...} вместо контента). Это привело к попаданию в базу \enquote{мусорных} документов.

\textbf{Решение:}
\begin{enumerate}
    \item \textbf{Очистка данных}: Написан вспомогательный скрипт, удаливший из MongoDB документы, содержащие фразу-маркер блокировки.
    \item \textbf{Доработка логики}: В робота внедрена функция \texttt{is\_banned(html)}, анализирующая ответ сервера.
    \begin{itemize}
        \item Для OpenNet - поиск специфических фраз (\enquote{Flood detected}, \enquote{Stop it}).
        \item Для Habr - обработка HTTP-кодов 429 и 503, а также проверка на наличие заглушек Qrator/DDoS-Guard.
        \item При обнаружении бана робот автоматически приостанавливает работу на 10 минут.
    \end{itemize}
    \item \textbf{Адаптивные задержки}: Для каждого источника настроены индивидуальные задержки, к которым добавляется случайное отклонение (jitter +-30\%) для имитации поведения человека.
\end{enumerate}

\textbf{Особенности источников:}
\begin{itemize}
    \item Для Habr.com добавлен параметр \texttt{step: 2}, так как статьи располагаются только на четных ID, что позволило ускорить сбор в два раза.
    \item Для OpenNet увеличена пауза между запросами до 5-6 секунд из-за строгой политики фаервола.
\end{itemize}

\newpage

\section{ЛР №3-5. Токенизация. Стемминг. Закон Ципфа}

\subsection{Токенизация}
\subsubsection{Методика токенизации}
Для разбиения текста на токены был разработан алгоритм, учитывающий особенности технического текста. Использован посимвольный проход с анализом контекста.

\textbf{Правила выделения токенов:}
\begin{itemize}
    \item \textbf{Базовое правило}: Любая последовательность букв или цифр считается частью токена.
    \item \textbf{Спецсимволы-разделители}: Все символы, кроме букв и цифр, считаются разделителями, за исключением следующих случаев:
    \begin{enumerate}[label=\alph*)]
        \item Точка (.) - считается частью токена, если находится между двумя цифрами или буквами (сохраняет целостность версий 2.0, IP-адресов 127.0.0.1 и дробных чисел 3.14). Точка в конце предложения отсекается.
        \item Дефис (-) - считается частью токена, если находится между двумя буквами (wi-fi, back-end).
        \item Плюс (+) - считается частью токена, если стоит после буквы или другого плюса (C++, g++, notepad++).
        \item Подчеркивание (\_) - сохраняется внутри слов (для переменных snake\_case).
    \end{enumerate}
\end{itemize}

\textbf{Приведение к нижнему регистру:}\\
Реализована собственная функция обработки UTF-8 строк, потому что стандартная функция \texttt{tolower} не работает с многобайтовой кириллицей.

\subsubsection{Статистические результаты}
На обработанном корпусе из $\sim$30 000 документов получены следующие данные:
\begin{itemize}
    \item Количество токенов: 74 350 557
    \item Средняя длина токена в символах: 5.29
    \item Средняя длина токена в байтах: 8.79
    \item Скорость обработки: $\sim$115 MB/sec.
\end{itemize}

\subsubsection{Достоинства и недостатки}
\begin{itemize}
    \item \textbf{Достоинства}: Высокая скорость, корректная обработка специфичных IT-терминов (C++, .net).
    \item \textbf{Недостатки}: Не распознаются сложные URL-адреса (они разбиваются на домены), не обрабатываются сокращения (например, т.е.).
\end{itemize}

\subsection{Закон Ципфа}
\subsubsection{Анализ распределения}
Для корпуса был построен частотный словарь и график рангового распределения в двойном логарифмическом масштабе.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Picture1.png}
    \caption{График закона Ципфа}
    \label{fig:zipf}
\end{figure}

\textbf{Результаты:}
\begin{itemize}
    \item График подтверждает закон Ципфа: наблюдается линейное убывание частоты от ранга ($\log(Freq) \sim -\log(Rank)$).
    \item \textbf{Отклонение}: Реальная кривая (синяя) проходит выше идеальной прямой Ципфа (красная). Это объясняется высокой плотностью терминологии в IT-корпусе. Словарь \enquote{средней частоты} (ранги 100-5000) используется активнее, чем в общелитературном языке, что дает более пологое падение кривой.
\end{itemize}

\subsubsection{Закон Мандельброта}
Для аппроксимации начального участка графика (топ-100 самых частых слов) были подобраны параметры закона Мандельброта:
\begin{equation}
    P(r) \propto \frac{1}{(r + \beta)^\alpha}
\end{equation}
Константы $\alpha = 1.05$, $\beta = 10.0$. Введение параметра смещения $\beta$ позволило сгладить шапку графика, где закон Ципфа давал слишком резкое падение.

\subsubsection{Статистический анализ словаря}
На основе частотного анализа корпуса получены следующие контрольные точки распределения:

\textbf{Топ частотных слов:}
\begin{itemize}
    \item Ранг 1: \enquote{и} - 1 644 619 вхождений.
    \item Ранг 10: \enquote{для} - 574 915 вхождений.
    \item Ранг 100: \enquote{без} - 85 626 вхождений.
\end{itemize}
Топ списка занимают союзы и предлоги. Это соответствует теории информационного поиска: самые частотные слова несут наименьшую смысловую нагрузку.

\textbf{Слова с частотой 1:}
\begin{itemize}
    \item Количество: 766 409 слов.
    \item Доля от словаря: 53.27\%.
\end{itemize}
Более половины уникальных термов в индексе встречаются в корпусе всего один раз. Это демонстрирует длинный хвост распределения и указывает на потенциальную возможность сжатия индекса путем отсечения слишком редких слов без существенной потери качества поиска.

\subsection{Стемминг}
Вместо полной лемматизации был реализован простой алгоритм стемминга с отсечением суффиксов и окончаний.
Алгоритм работает по жадному принципу: проверяется наличие окончания из предопределенного списка для русского и английского, начиная с самых длинных (-вшимися, -ization) до самых коротких (-а, -s).

\subsubsection{Оценка качества поиска}
Было проведено сравнение количества найденных документов по точному совпадению и после применения стемминга.

\textbf{Пример: Запрос \enquote{система}}
\begin{itemize}
    \item Точное совпадение: 8074 документов.
    \item Со стеммингом (систем): 20 569 документов.
    \item Результат: Найдены словоформы системы, системе, системам, системах, систему. Реколл значительно улучшился.
\end{itemize}

\textbf{Пример: Запрос \enquote{банка}}
\begin{itemize}
    \item Точное совпадение: 826 документов.
    \item Со стеммингом (систем): 2 252 документов.
    \item Результат: Найдены словоформы банк, банку, банков, банке, банках.
\end{itemize}

\textbf{Проблема:} Стеммер может ошибочно объединять разные по смыслу слова. Например \textit{банка} (стеклянная) может быть урезана до \textit{банк}, что приведет к смешиванию контекстов.

\textbf{Вывод:} Внедрение стемминга критически важно для поиска по русскому языку из-за богатой морфологии. Это увеличивает полноту поиска в 2-3 раза, хотя и незначительно снижает точность на омонимах.

\newpage

\section{ЛР №6. Булев индекс}

\subsection{Формат индекса}
Для хранения поискового индекса был разработан собственный бинарный формат данных, обеспечивающий компактное хранение и быстрый доступ без использования СУБД. Индекс разделен на два файла. Используется порядок байт Little-Endian.

\textbf{А. Прямой индекс (docs.bin)}
Служит для получения мета-данных (URL, Заголовок) по идентификатору документа (DocID).

Структура файла (побайтово):
\begin{itemize}
    \item \textbf{Header} (4 байта) - TotalDocs (uint32) - общее количество документов.
    \item \textbf{Offset Table} (N * 8 байт) - Массив чисел uint64. Значение Offset[i] указывает на абсолютную позицию начала данных для документа с ID=i.
    \item \textbf{Data Area}: Последовательность записей. Формат одной записи:
    \begin{itemize}
        \item (a) UrlLen (2 байта, uint16).
        \item (b) URL (UrlLen байт, ASCII).
        \item (c) TitleLen (2 байта, uint16).
        \item (d) Title (TitleLen байт, UTF-8).
    \end{itemize}
\end{itemize}

\textbf{Б. Обратный индекс (index.bin)}
Связывает термы с идентификаторами документов, в которых они встречаются.

Структура файла:
\begin{enumerate}
    \item \textbf{Header} (12 байт):
    \begin{itemize}
        \item TotalTerms (4 байта, uint32) - количество уникальных слов.
        \item DictSize (8 байт, uint64) - размер секции словаря в байтах.
    \end{itemize}
    \item \textbf{Dictionary Section}: Список словарных статей. Записи идут подряд переменной длины:
    \begin{itemize}
        \item TermLen (1 байт, uint8).
        \item Term (TermLen байт, UTF-8).
        \item DocFreq (4 байта, uint32) - количество документов, содержащих термин.
        \item PostingsOffset (8 байт, uint64) - смещение начала списка DocID относительно начала секции постингов.
    \end{itemize}
    \item \textbf{Postings Section}: Сплошной массив DocID (uint32). Списки для разных слов идут друг за другом.
\end{enumerate}

\subsection{Алгоритм построения (BSBI)}
Использован подход Block Sort-Based Indexing.
\begin{itemize}
    \item \textbf{Токенизация}: Весь корпус обрабатывается потоково. Пары (Token, DocID) сохраняются в единый массив структур в оперативной памяти.
    \item \textbf{Сортировка}: Массив сортируется лексикографически по токену, а затем по DocID. Использован алгоритм quicksort, сложность $O(N \log N)$.
    \item \textbf{Сжатие}: Повторяющиеся пары удаляются (\texttt{std::unique}), формируя уникальные вхождения.
    \item \textbf{Сброс на диск}: Отсортированные данные последовательно записываются в \texttt{index.bin}.
\end{itemize}

\textbf{Достоинства метода:}
\begin{itemize}
    \item Высокая скорость работы в памяти.
    \item Последовательная запись на жесткий диск (избегает random seek), что оптимально для HDD/SSD.
    \item Отсутствие сложных динамических структур (деревьев), что упрощает код и уменьшает оверхед по памяти.
\end{itemize}

\textbf{Недостатки:}
\begin{itemize}
    \item Требует загрузки всех пар токен-документ в RAM. При превышении объема физической памяти потребуется переход на SPIMI (Single-Pass In-Memory Indexing).
\end{itemize}

\textbf{Масштабируемость}: Сейчас алгоритм ограничен объемом RAM. Что если данных в 1000 раз больше?
\begin{itemize}
    \item Ответ: Оперативная память закончится.
    \item Решение: SPIMI. Строить индекс кусками по 1 ГБ, сохранять их на диск, а потом сливать (Merge Sort) несколько файлов индексов в один итоговый.
\end{itemize}

\newpage

\section{ЛР №7. Булев поиск}

\subsection{Архитектура системы}
Поисковая система реализована в виде двух компонентов:
\begin{itemize}
    \item \textbf{Backend}: Высокопроизводительная утилита на C++. Загружает индексы в память и выполняет математические операции над множествами. Использует бинарный поиск по словарю для нахождения терминов.
    \item \textbf{Frontend}: Веб-сервер на Python + Flask. Предоставляет графический интерфейс, принимает запросы пользователя и отображает результаты.
\end{itemize}
Взаимодействие с backend происходит через CLI-аргументы.

\subsection{Обработка запросов}
Парсер поддерживает операторы \&\&, ||, !, ( ).

\textbf{Алгоритм:}
\begin{itemize}
    \item \textbf{Препроцессинг}: Вставка неявных операторов AND (между двумя словами подряд).
    \item \textbf{Shunting-yard}: Преобразование инфиксной нотации (человеко-читаемой) в обратную польскую запись. Пример:
    \begin{itemize}
        \item Вход: \texttt{(A || B) \&\& !C}
        \item RPN: \texttt{A B || C ! \&\&}
    \end{itemize}
    \item \textbf{Выполнение}: Стековая машина вычисляет результат, используя операции над сортированными списками:
    \begin{itemize}
        \item AND - пересечение (Intersection, линейный проход).
        \item OR - объединение (Union, линейный проход).
        \item NOT - разность множества всех документов и текущего списка.
    \end{itemize}
\end{itemize}

\subsection{Тестирование и Производительность}
\textbf{Скорость поиска}: Время выполнения булевых операций составляет $< 10$ мс. Основное время затрачивается на извлечение заголовков документов с диска для отображения сниппетов.

\textbf{Тестовые сценарии:}

\texttt{московский авиационный институт} (эквивалентно AND) - проверено, что находятся документы, где есть все три слова.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture2.png}
    \caption{Демонстрация поиска}
    \label{fig:search1}
\end{figure}

\texttt{java || python} - количество результатов совпадает с суммой уникальных документов по каждому слову.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture3.png}
    \caption{Демонстрация запроса \enquote{java || python}}
    \label{fig:search2}
\end{figure}

\texttt{руки !ноги} - проверено отсутствие слова \enquote{ноги} в найденных документах.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture4.png}
    \caption{Демонстрация запроса \enquote{руки}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture5.png}
    \caption{Демонстрация запроса \enquote{руки\&\&ноги}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture6.png}
    \caption{Демонстрация запроса \enquote{руки ноги}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Picture7.png}
    \caption{Демонстрация запроса \enquote{руки !ноги}}
\end{figure}

По приложенным демонстрационным скриншотам видно, что количество найденных документов меняется в соответствии с нашими запросами.

\clearpage 

\section*{Вывод}
В ходе выполнения цикла лабораторных работ была спроектирована и реализована полнофункциональная поисковая система, поддерживающая булеву логику запросов. Был пройден полный цикл разработки: от сбора сырых данных до реализации веб-интерфейса и своего бинарного формата хранения индекса.

Поисковый движок, написанный на C++, продемонстрировал высокую производительность. Использование бинарного поиска по словарю и алгоритма сортировочной станции для парсинга запросов обеспечивает время отклика менее 10 мс даже на сложных булевых выражениях с вложенными скобками.

В результате работы создана масштабируемая архитектура поисковой системы. Система устойчива к росту объема данных в пределах оперативной памяти. Для дальнейшего масштабирования архитектура позволяет легко перейти от BSBI к алгоритму SPIMI и распределенному поиску, так как формат бинарных блоков уже оптимизирован для последовательного слияния.

\end{document}
